{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be548dd0",
   "metadata": {},
   "source": [
    "### DeepEval configuration Guide\n",
    "\n",
    "This notebook demonstrates the basic usage of the `deepeval` library. We'll cover:\n",
    "\n",
    "- Logging test cases  \n",
    "- Running evaluations  \n",
    "- Viewing and saving results locally  \n",
    "- Evaluating DeepEval metrics through the Trace metrics API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c6ca9",
   "metadata": {},
   "source": [
    "## RAG Test Case: \n",
    "\n",
    "In this example, we define a **Retrieval-Augmented Generation (RAG)** test case using `deepeval`. The goal is to evaluate how well a language model's response aligns with both the expected output and the retrieved context.\n",
    "\n",
    "### What We're Doing\n",
    "\n",
    "- **Input**: A user asks _\"What causes seasonal color changes in leaves?\"_\n",
    "- **Actual Output**: The model's generated response.\n",
    "- **Expected Output**: A reference answer used for comparison.\n",
    "- **Context**: The full context provided to the model for generation.\n",
    "- **Retrieval Context**: The subset of documents retrieved for grounding the answer.\n",
    "\n",
    "We use `LLMTestCase` to encapsulate this information, which will later be evaluated using various DeepEval metrics such as:\n",
    "- `AnswerRelevancyMetric`\n",
    "- `ContextualRelevancyMetric`\n",
    "- `ContextualRecallMetric`\n",
    "- `ContextualPrecisionMetric`\n",
    "- `FaithfulnessMetric`\n",
    "- `HallucinationMetric`\n",
    "\n",
    "This setup allows us to assess factual consistency, grounding, and hallucination risk in RAG-based systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3582124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric\n",
    ")\n",
    "\n",
    "# Define RAG test case with context and retrieval_context\n",
    "tc = LLMTestCase(\n",
    "    input=\"What causes seasonal color changes in leaves?\",\n",
    "    actual_output=\"Leaves change color due to reduced chlorophyll production in fall, revealing carotenoids and anthocyanins. Temperature and light changes trigger this process.\",\n",
    "    expected_output=\"Seasonal leaf color changes are primarily caused by the breakdown of chlorophyll in autumn, revealing underlying pigments like carotenoids (yellows/oranges) and anthocyanins (reds/purples), triggered by shorter days and cooler temperatures.\",\n",
    "    context=[\n",
    "        \"Photosynthesis slows in autumn due to reduced sunlight and temperature changes.\",\n",
    "        \"Chlorophyll breaks down faster than it's produced, unmasking existing carotenoids.\",\n",
    "        \"Anthocyanins are newly synthesized in some species as sugars become trapped in leaves.\",\n",
    "        \"The process is influenced by both photoperiod (day length) and temperature changes.\"\n",
    "    ],\n",
    "    retrieval_context=[\n",
    "        \"Photosynthesis slows in autumn due to reduced sunlight and temperature changes.\",\n",
    "        \"Chlorophyll breaks down faster than it's produced, unmasking existing carotenoids.\",\n",
    "        \"Anthocyanins are newly synthesized in some species as sugars become trapped in leaves.\",\n",
    "        \"The process is influenced by both photoperiod (day length) and temperature changes.\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472625d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=\"Your OpenAI API Key Here\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c26cb4",
   "metadata": {},
   "source": [
    "## Metric Evaluation\n",
    "\n",
    "In this step, we initialize a set of evaluation metrics with custom thresholds and apply them to our RAG test case.\n",
    "\n",
    "### Metrics Used\n",
    "\n",
    "- `AnswerRelevancyMetric (≥ 0.7)`: Measures how relevant the model's answer is to the input question.\n",
    "- `ContextualRelevancyMetric (≥ 0.8)`: Evaluates how well the answer relates to the provided context.\n",
    "- `ContextualRecallMetric (≥ 0.9)`: Measures how much relevant information from the context is included in the output.\n",
    "- `ContextualPrecisionMetric (≥ 0.85)`: Measures how much of the output is grounded in the relevant context.\n",
    "- `FaithfulnessMetric (≥ 0.9)`: Checks whether the generated answer is faithful to the source context.\n",
    "- `HallucinationMetric (≤ 0.1)`: Detects content in the answer that is not supported by the context.\n",
    "\n",
    "### Evaluation Loop\n",
    "\n",
    "Each metric is applied to the test case using the `measure()` method. The results are stored in a list as dictionaries containing:\n",
    "- `metric_key`: The name of the metric\n",
    "- `value`: The computed score\n",
    "\n",
    "These results can be used for reporting, logging, or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics with appropriate thresholds\n",
    "metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7),\n",
    "    ContextualRelevancyMetric(threshold=0.8),\n",
    "    ContextualRecallMetric(threshold=0.9),\n",
    "    ContextualPrecisionMetric(threshold=0.85),\n",
    "    FaithfulnessMetric(threshold=0.9),\n",
    "    HallucinationMetric(threshold=0.1)\n",
    "]\n",
    "\n",
    "# Evaluate all metrics\n",
    "metric_results = {}\n",
    "for m in metrics:\n",
    "    m.measure(tc)\n",
    "    metric_results[m.__class__.__name__] = m.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c73fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AnswerRelevancyMetric': 1.0, 'ContextualRelevancyMetric': 1.0, 'ContextualRecallMetric': 1.0, 'ContextualPrecisionMetric': 1.0, 'FaithfulnessMetric': 1.0, 'HallucinationMetric': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(metric_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0640f",
   "metadata": {},
   "source": [
    "## How to get your subscription key for the TRACE Metrics API\n",
    "\n",
    "To access the TRACE Metrics API, you need a **subscription key**. Follow these steps:\n",
    "\n",
    "1. **Sign in to CognitiveView**  \n",
    "   Go to [app.cognitiveview.com](https://app.cognitiveview.com) and log in with your account credentials.\n",
    "\n",
    "2. **Navigate to System Settings**  \n",
    "   Once logged in, find the **System Settings** option in the main menu.\n",
    "\n",
    "3. **Generate and view your subscription key**  \n",
    "   - Inside **System Settings**, look for the section labeled **API Access** or **Subscription Key**.\n",
    "   - If a key has already been generated, you can copy it directly.\n",
    "   - If not, click the **Generate Key** button to create a new key.\n",
    "\n",
    "4. **Copy and save your subscription key**  \n",
    "   Keep this key secure. You’ll need to include it in your API requests for authentication.\n",
    "\n",
    "---\n",
    "\n",
    "## Posting Evaluation Metrics to TRACE Metrics API\n",
    "\n",
    "This script sends your evaluation metric results (for example, from DeepEval) to the TRACE Metrics API using an authenticated HTTP POST request.\n",
    "\n",
    "### Authentication\n",
    "\n",
    "- Use an **Authorization token** (`AUTH_TOKEN`) in the request header.\n",
    "- Include an **X-User-Id** header to identify the user making the request.\n",
    "\n",
    "---\n",
    "\n",
    "### Endpoint\n",
    "\n",
    "| Item         | Value                                                      |\n",
    "|--------------|-----------------------------------------------------------|\n",
    "| **Base URL** | `https://api.cognitiveview.com`                            |\n",
    "| **API Path** | `/cv/v1/metrics`                                          |\n",
    "| **Full URL** | `https://api.cognitiveview.com/cv/v1/metrics`             |\n",
    "\n",
    "---\n",
    "\n",
    "### Payload Structure\n",
    "\n",
    "#### `metric_metadata`\n",
    "\n",
    "Contains information about what you’re evaluating:\n",
    "- `application_name`: Name of the evaluated application.\n",
    "- `version`: Version of the application or model.\n",
    "- `provider`: The metric system or platform (for example, `deepeval`).\n",
    "- `use_case`: The business or functional domain (for example, `customer_support`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801caffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_metrics_to_TRACE_Metric_API(metric_results, auth_token, user_id):\n",
    "  \"\"\"\n",
    "  Posts DeepEval metric results to the Trace Metric API.\n",
    "\n",
    "  Args:\n",
    "    metric_results (dict): Dictionary of computed metric scores.\n",
    "    auth_token (str): Authorization token for the API.\n",
    "    user_id (str): User ID for the API (default is \"C473421_T181751\").\n",
    "\n",
    "  Returns:\n",
    "    dict: Response JSON from the API.\n",
    "  \"\"\"\n",
    "  import requests\n",
    "\n",
    "  BASE_URL = \"https://api.cognitiveview.com\"\n",
    "  url = f\"{BASE_URL}/cv/v1/metrics\"\n",
    "\n",
    "  headers = {\n",
    "    \"Authorization\": auth_token,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-User-Id\": user_id,\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "    \"metric_metadata\": {\n",
    "    \"application_name\": \"chat-application\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"url\": \"https://api.example.com/chat\",\n",
    "    \"provider\": \"deepeval\",\n",
    "    \"use_case\": \"transportation\"\n",
    "    },\n",
    "    \"metric_data\": {\n",
    "    \"deepeval\": metric_results\n",
    "    } \n",
    "  }\n",
    "\n",
    "  response = requests.post(url, headers=headers, json=payload)\n",
    "  print(f\"Status Code: {response.status_code}\")\n",
    "  try:\n",
    "    print(\"Response JSON:\", response.json())\n",
    "    return response.json()\n",
    "  except Exception:\n",
    "    print(\"Response Text:\", response.text)\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "AUTH_TOKEN = \"Your-Authorization-Token-Here\"  # Replace with your actual token\n",
    "user_id = \"user_id\"  # Replace with your actual user ID\n",
    "post_metrics_to_TRACE_Metric_API(metric_results,AUTH_TOKEN,user_id)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
